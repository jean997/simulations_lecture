---
title: "Simulations"
author: "Jean Morrison"
date: "March 24, 2022 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [robot-fonts, robot, "scroll.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      scroll: false
---
# Simulation

- A **simulation** is a computational experiment. 
  
- The simulations we will talk about today are **Monte Carlo** simulations. 
  + The term Monte Carlo is used for methods that repeatedly sample from a probability distribution. 
  
- Simulating is an incredibly powerful statistical technique with very broad application. 

- Simulating is a strategy for estimating parameters that we cannot derive or approximate theoretically. 



```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
xaringanExtra::use_search(show_icon = TRUE)
xaringanExtra::use_panelset()
```

---

# Use Cases

- Estimating properties of estimators

  + An estimator we want to use does not have known mean/variance/etc.
  + We want to assess finite-sample performance. 
  + We want to see what happens if assumptions are violated. 

--

- Estimating power or necessary sample size
  + If we run this experiment, what are the chances that we can reject the null hypothesis?

--

- Comparing multiple estimation strategies
  + Methods A, B, and C all estimate the same parameter. Which is best?

--

- As a research and learning tool
  + Demonstrate a concept to yourself. 
  + Quickly assess if new results are reasonable. 
  + Test that software is working as expected.  

---

# Properties of Estimators

- Most of our favorite estimators have known theoretical distributions. 

- For example, the one-sample t-statistic 
$$ t(X_1, \dots, X_N) = \frac{\bar{X}}{\hat{\sigma}/\sqrt{N}}$$
is *exactly* t-distributed with mean $E[X_i]$ and $N-1$ degrees of freedom if $X_1, \dots, X_N$ are independent, identically distributed (iid), normal random variables. 

- This distribution is a good approximation as long as $X_1, \dots, X_n$ are iid with finite mean and variance, and $N$ is "not too small". 

- We can use the theoretical t-distribution to produce confidence intervals and $p$-values testing that the mean is equal to 0. 

---
# Properties of Estimators

- In some cases, we are interested in estimators with unknown distribution (nobody has derived it or it is very hard to write down). 

- We might want to know the *finite-sample* properties of an estimator that has known asymptotic distribution. 
  + What is the distribution of the t-test if $N$ is small and the sample is not normal?
  
+ We might want to know properties of the estimator when some of the assumptions used to derive a theoretical distribution are violated. 


---
# Example 1: Welch's t-statistic


- Suppose we want to estimate the difference in means of two populations using samples  $\mathbf{X}_1 = (X_{1,1}, \dots, X_{N_1,1})$ and $\mathbf{X}_2 = (X_{1, 2}, \dots, X_{N_2, 2})$. 
  + True parameter $\mu = E[X_1]- E[X_2]$

- One estimator is the difference in sample means $\hat{\mu} \equiv \bar{X}_1 - \bar{X}_2$

- The Welch's two-sample t-statistic is $$\hat{t} = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s^2_1}{N_1} + \frac{s^2_2}{N_2}}}$$


---
# Example 1: Welch's t-statistic

- The t-distribution with $N_1 + N_2 -2$ degrees of freedom is a good approximation for the distribution of $\hat{t}$ if the samples are normal or sample size is large and $\mu = 0$. 

- We can use this result to get a confidence interval for $\hat{\mu}$ as $(\hat{L}, \hat{U})$ where 
  + $\hat{L} = \hat{\mu} - t_{\alpha/2}\sqrt{\frac{s^2_1}{N_1} + \frac{s^2_2}{N_2}}$
  + $\hat{U} = \hat{\mu} + t_{1-\alpha/2}\sqrt{\frac{s^2_1}{N_1} + \frac{s^2_2}{N_2}}$

- How worried should we be about non-normal data?

- How many samples is "enough" for the t-distribution to be a "good" approximation?

---
# Example 1: Strategy

1. Generate data computationally (e.g. in R) for two samples.
  + This is a "synthetic" or "simulated" data set.
  + Call it $D_j$. The $j$ indexes the simulation number.

2. Analyze the data and store the result.  
  + Compute the difference in means, the t-statistics, and the confidence interval based on the t-distribution approximation.

3. Evaluate the result
  + Compute some measures of how we can estimate the difference in means. 
  
+ Repeat steps 1,2, and 3 many times (for $j = 1, \dots, J$). 

+  Summarize the results. 

---
# Simulation Settings

- We need to decide *how* to create our synthetic data set. 
  + We said we wanted to know what happened when the data were non-normal. 
  + But there are lots of ways to generate non-normal data. 
  
- We might want to compare a few different choices of simulation strategy. 
  + Different distributions 
  + Different sample sizes

- These are called *simulation settings*. 
  + There are an infinite number of possible settings so we will have to just choose a few that seem interesting. 
  
---
# Example 1: Settings

+ Setting 1: 
  + Sample 1 and Sample 2 are both drawn from a binomial distribution with mean 0.25.  
  + The two samples have equal mean and equal variance but are not normal.

+ Setting 2: 
  + Sample 1 is drawn from a negative binomial distribution with mean 10 and over-dispersion parameter 0.1. 
  + Sample 2 is drawn from a negative binomial distribution with mean 10 and over-dispersion parameter 1. 
  + Samples have equal mean but unequal variance and are not normal.
  
- In both settings, we will look at a range of sample sizes.

  +  Consider $N_1 = N_2 = 10, 20, 50, 100, 500$, and 1000 observations. 

---
# Example 1: Analysis

- Once we have a simulated data set, we need to apply our estimator to it. 

- Compute $\hat{\mu}(D_j)  = \hat{\mu}_j$

- Computing the t-statistic, $\hat{t}(D_j) = \hat{t}_j$.

- Compute the usual t-test confidence interval $(\hat{L}_j, \hat{U}_j)$. 

---
# Example 1: Evaluation

- To get something concrete out of the experiments, we need to be able to quantify our questions: 

  + "How worried should we be about non-normal data?"
  + "How many samples is enough?"

--

- We can look to the properties that would make us "not worried". 

  - We want $\hat{\mu}$ to have expected value equal to $E[X_1] - E[X_2]$. 
  - We want the 95% confidence interval based on the t-distribution to contain the true difference in means 95% of the time. 
  - We want the $p$-value based on comparing $\hat{t}_j$ to the t-distribution to be uniformly distributed if $\mu = 0$ (won't look at today).

---
# Example 1: Evaluation

- For each simulated data set we need some evaluation metrics that tell us how well the t-statistic is estimating the difference in means. 

- Compute the bias $\hat{t}_j - \mu$ where $\mu = E[X_1]-E[X_2]$. 
  + In our settings $\mu$ is always 0.
  
- Compute the coverage: 
  + Is $L_j < \mu < U_j$?
  
- Could also compute the $p$-value $p_j$.

---
# Example 1: Simluation Code

- I strongly recommend writing functions when you run simulations. 

- The functions below generate data from Setting 1 and Setting 2. 

- Notice that means and over-dispersion parameters are function parameters rather than being "hard-coded".
  + This makes it easy to modify settings quickly.

```{r}
library(stats) # For NB distribution
sim_setting1 <- function(n1, n2, prob1 = 0.25, prob2 = 0.25){
  x1 <- rbinom(n = n1, size = 1, prob = prob1)
  x2 <- rbinom(n = n2, size = 1, prob = prob2)
  return(list(x1 = x1, x2 = x2, mu = prob1-prob2))
}

sim_setting2 <- function(n1, n2, mean1 = 10, mean2 = 10, od1 = 0.1, od2 = 1){
  x1 <- rnbinom(n = n1, mu = mean1, size = 1/od1)
  x2 <- rnbinom(n = n2, mu = mean2, size = 1/od2)
  return(list(x1 = x1, x2 = x2, mu = mean1-mean2))
}
```

---
# Example 1: Analysis Code

- The analysis part of this experiment is just a t-test

```{r}
my_t_test <- function(D){
  x <- c(D$x1, D$x2)
  grp <- rep(c(0, 1), c(length(D$x1), length(D$x2)))
  f <- t.test(x ~ grp)
  mu <- as.numeric(f$estimate[1]) - as.numeric(f$estimate[2])
  return(list(t = f$statistic, 
              mu = mu,
              L = f$conf.int[[1]], 
              U = f$conf.int[[2]], 
              p = f$p.value))
}
```


---
# Example 1: Evaluate Code

- For each data set we have two evaluation metrics we want to compute: 

- Bias $\hat{\mu}_j - 0$ and coverage, $1_{L_j < \mu < U_j}$.

- These are simple enough that we could have just added them into the `my_t_test` function. 

- I am giving them their own functions in order to make a point later about the structure of simulation experiments. 


```{r}

bias <- function(D, result){
  result$mu-D$mu
}

covered <- function(D, result){
  result$L < D$mu & result$U > D$mu
}

```

---
# Example 1: Test

Make sure that all the code works as expected. 

```{r}
set.seed(123)
Dtest <- sim_setting1(n1=20, n2 = 20)

rtest <- my_t_test(Dtest)
rtest

bias(Dtest, rtest)

covered(Dtest, rtest)
```

---
# Example 1: Run Simulations

- Often your simulations will be in a big `for` loop. 
  + In R, functions like `map`, `lapply`, and `replicate1` are all useful. 
  
- Sometimes there will be layers of loops: 
  - Outside loop: simulation parameters (sample size)
  - Inside loop: simulation replicates
  
```{r, warning=FALSE, message=FALSE}
library(purrr)
library(dplyr)
library(ggplot2)
```
  
```{r, cache = TRUE}
set.seed(1) # Important!
nsims <- 1000
sample_size <- c(10, 20, 50, 100, 500, 1000)

setting1 <- map_dfr(sample_size, function(n){ 
  # Sample size loop
  X <- map_dfr(seq(nsims), function(j){
      # Replication loop
      
      # Simulate
      D <- sim_setting1(n1 = n, n2 = n, prob1 = 0.25, prob2 = 0.25)
      # Analyse
      result <- my_t_test(D)
      # Evaluate
      b <- bias(D, result)
      c <- covered(D, result)
      # Return
      data.frame(bias = b, covered = c, 
           p = result$p, t = result$t, 
           sample_size = n, j = j)
    })
  return(X)
})

setting2 <- map_dfr(sample_size, function(n){ 
  # Sample size loop
  X <- map_dfr(seq(nsims), function(j){
      # Replication loop
      
      # Simulate
      D <- sim_setting2(n1 = n, n2 = n, 
                        mean1 = 10, mean2 = 10, 
                        od1 = 0.1, od2 = 10)
      # Analyse
      result <- my_t_test(D)
      # Evaluate
      b <- bias(D, result)
      c <- covered(D, result)
      # Return
      data.frame(bias = b, covered = c, 
           p = result$p, t = result$t, 
           sample_size = n, j = j)
    })
  return(X)
})

```

---
# Example 1: Run Simulations

- The result of running the simulations is a big data frame with 6000 rows.


```{r}
dim(setting1)
head(setting1)
table(setting1$sample_size)
```


---
# Example 1: Summarize

- We have our experiment results, now we need to summarize them. 

- Below I compute average bias, coverage rate, and the standard deviation of the t-statistics. 

- The standard deviation of the $\hat{t}_j$'s is an estimate of the standard error of the t-statistic.
  + Should be close to $\frac{N-2}{N-4}$ where $N = N_1 + N_2$. 

```{r, message=FALSE, warning=FALSE}
result_tab1 <- setting1 %>% group_by(sample_size) %>%
    summarize(avg_bias = mean(bias, na.rm=TRUE), 
              coverage = mean(covered, na.rm=TRUE), 
              sd_t = sd(t, na.rm=TRUE), 
              nmiss = sum(is.na(bias))) %>%
    mutate(Setting = "Setting 1")

result_tab2 <- setting2 %>% group_by(sample_size) %>%
    summarize(avg_bias = mean(bias, na.rm=TRUE), 
              coverage = mean(covered, na.rm=TRUE), 
              sd_t = sd(t, na.rm=TRUE), 
              nmiss = sum(is.na(bias)))%>%
    mutate(Setting = "Setting 2")
  
result_tab <- bind_rows(result_tab1, result_tab2) 
```

---
# Example 1: Average Bias

```{r, fig.align='center', fig.width=10}
result_tab %>% ggplot(aes(x = sample_size, y = avg_bias)) + 
    geom_point(aes(color = Setting, shape = Setting), size = 5) + 
    geom_line(aes(color = Setting, linetype = Setting, group = Setting), size = 1.5) +
    geom_hline(yintercept = 0) + 
    scale_x_log10() + 
    xlab("Sample Size") + 
    ylab("Average Bias") + 
    theme_bw() + 
    theme(axis.text = element_text(size = 16), 
          legend.text = element_text(size =16), 
          axis.title = element_text(size = 18), 
          legend.title = element_text(size = 18))
```


---
# Example 1: Coverage

```{r, fig.align='center', fig.width=10}
result_tab %>% ggplot(aes(x = sample_size, y = coverage)) + 
    geom_point(aes(color = Setting, shape = Setting), size = 5) + 
    geom_line(aes(color = Setting, linetype = Setting, group = Setting), size = 1.5) +
    geom_hline(yintercept = 0.95) + 
    scale_x_log10() + 
    xlab("Sample Size") + 
    ylab("Average Coverage of 95% CI") + 
    theme_bw() + 
    theme(axis.text = element_text(size = 16), 
          legend.text = element_text(size =16), 
          axis.title = element_text(size = 18), 
          legend.title = element_text(size = 18))
```

---
# Example 1: Standard Error



```{r, fig.align='center', fig.width=10}
result_tab %>% ggplot(aes(x = sample_size, y = sd_t)) + 
    geom_point(aes(color = Setting, shape = Setting), size = 5) + 
    geom_line(aes(color = Setting, linetype = Setting, group = Setting), size = 1.5) +
    geom_line(aes(x = sample_size, y = (2*sample_size-2)/(2*sample_size -4)))+ 
    scale_x_log10() + 
    xlab("Sample Size") + 
    ylab("SD of t-stats") + 
    theme_bw() + 
    theme(axis.text = element_text(size = 16), 
          legend.text = element_text(size =16), 
          axis.title = element_text(size = 18), 
          legend.title = element_text(size = 18))
```

---
# Example 1: Conclusions

- The t-test is performing very well in the binomial data, even at small sample size. 
  + The bias is low
  + Coverage is close to expected.
  + Slight over-coverage suggests that tests of the non-null could be a little bit conservative. 
  
- The t-test did not do well in small sample sizes in setting 2 where the distributions of Sample 1 and Sample 2 had very different variances. 
  + Bias is large and coverage is low. 
  + We estimate that the t-statistic will have a larger variance than predicted by the t-distribution approximation. 
  + The t-test improves as sample size increases (as predicgted by the central limit theorem).
  
---
# All Simulations Have the Same Components

- Create data: A method for creating synthetic data with some known properties

- Analyze: One or more methods to analyze the data (i.e. estimate a parameter)

- Evaluate: One or more functions to compare the results from the analysis step with the truth from the create data step. 

- Summarize: A way to summarize the evaluations of many simulated data sets. 



---
# Familiar Simulations

- There are two types of simulation you may already be familiar with: permutation testing, and the bootstrap.

- Both of these methods use a "non-parametric" simulation strategy. 

- Both of these methods fall into the "estimating properties of estimators" category.

- Use permutation testing to estimate the distribution of an estimator under the null. 

- Use the bootstrap to estimate the distribution of an estimator in the true data generating distribution. 


---
# Permutation Testing

- Suppose we have a data set $D_0$ which consists of $N$ observations $(Y_1, X_1), \dots, (Y_N, X_N)$.

- We are interested in testing the **null hypothesis that $Y$ and $X$ are independent**. 

- We have a test statistc $\hat{T}(D)$. 
  + For example, $\hat{T}$ might be the correlation between $Y$ and $X$. 
  
- In order to test the null hypothesis, we need to know the distribution of $\hat{T}$ under the null. 

- We might not have a good estimate of this distribution. 
  + For example, in Setting 2 in the previous example, the t-distribution was a bad approximation. 
  
---
# Permutation Testing

- If we could simulate data under the null, we could estimate the behavior of $\hat{T}$ under the null.

- Permutations are a non-parametric way to do this. 

- Start with the data we actually observed, $(Y_1, X_1), \dots (Y_N, X_N)$. 

- To simulate one new data set, sample a permutation $S_j$ from the set of all permuations of $N$ items. 

- The new data set is $D_j = (Y_{S_j(1)}, X_1), \dots, (Y_{S_j(N)}, X_N)$. 
  + We have sampled $Y$ from the empirical distribution of $Y$ found in our data. 
  + Likewise for $X$.
  + However, in our sampling scheme, $Y$ and $X$ are independent. 
  
---
# Permutation Testing

- For each simulated data set, $D_j$, we compute $\hat{T}(D_j) \equiv \hat{T}_j$.

- After many simulations, we count how many of the $\hat{T}_j$'s are above or below $\hat{T}(D_0)$
  
---
# Bootstrap

- The bootstrap is conceptually similar to permutations except that the focus is on estimating the distribution of the test statistic in our actual data rather than under the null.
  + If $\mathbb{P}$ is the true data distribution and $\mathbb{P}_0$ is the null data distribtuion, 
  + Permutation testing estimate the distribution of $T(\mathbb{P}_0)$
  + Bootstrapping estimates the distribution of $T(\mathbb{P})$
  

- We again want to sample from the empirical distribution of the data. 
  + Except this time we don't want to change anything or break any dependencies between variables. 
  
---
# Bootstrap

- Suppose that $D_0$ consists of $N$ independent observations, $D_0 = \lbrace X_1, \dots, X_N \rbrace$ 

- In each simulation, we sample, $N$ observations with replacement, placing equal probability on each observation. 

- In each bootstrap sample, $D_j$, we compute $\hat{T}(D_j) \equiv \hat{T}_j$.

- After many iterations, the empirical distribution of the $\hat{T}_j$'s will approximate the distribution of $T(\mathbb{P})$.

  
---
# Estimatimg Power

- A very common use of simulations is to estimate the chances of detecting a "significant" result in an experiment that hasn't been done yet. 

- In order to estimate power we need to know:

  + Approximately what the data will look like (will they be normal? binomial? something else?)
  + What effect size the investigators think is reasonable
  + What analysis strategy we are going to use the analyze the data when we get it. 
  
- With this information, we can go through the same simulation process we used previously:
  + Create data
  + Analyze
  + Evaluate -- in this step our evaluation is often whether or not $p_j < 0.05$ (or some other threshold).
  + Summarize: Over all the simulations, how many times was $p_j < 0.05$?
  
---
# Example 2: Power

- We will use the difference in means example again for simplicity. 

- Our investigators are going to collect data from two groups. 
  + They tell us that they expect both samples to be about normal. 
  + They expect sample 1 to have a variance of 1 and sample 2 to have a variance of 2. 
  + They think the difference in means could be between 0.1 and 0.5
  + How many samples do they need?
  
---
# Example 2: Functions

- The function below simulates data

```{r}
sim_normal <- function(n1, n2, mean_diff= 0.1, sd1 = 1, sd2 = sqrt(2)){
  x1 <- rnorm(n = n1, mean = 0, sd = sd1)
  x2 <- rnorm(n = n2, mean = mean_diff, sd = sd2)
  return(list(x1 = x1, x2 = x2, mu = mean_diff))
}
```

- We can use the same `my_t_test` function we used previously to analyze. 

- To evaluate, we need to know if the p-value is below a signficance threshold

```{r}
detected <- function(result, thresh){
  result$p < thresh
}
```

---
# Example 2: Run

- We now have two parameters that we might want to vary in the simulate step: 
  + Sample size and effect size

```{r, cache = TRUE}
set.seed(1) # Important!
nsims <- 1000
sample_size <- c(10, 20, 50, 100, 200, 500)
effect_size <- c(0.1, 0.3, 0.5)

settings <- expand.grid(ss = sample_size, e = effect_size)


normal_sims <- map2_dfr(settings$ss, settings$e, function(n, e){ 
  # Sample and effect size loop
  X <- map_dfr(seq(nsims), function(j){
      # Replication loop
      
      # Simulate
      D <- sim_normal(n1 =n, n2 = n, mean_diff = e)
      # Analyse
      result <- my_t_test(D)
      # Evaluate
      d <- detected(result, 0.05)
      # Return
      data.frame(detected = d,
           p = result$p, t = result$t, 
           sample_size = n, effect_size = e, j = j)
    })
  return(X)
})
table(normal_sims$sample_size, normal_sims$effect_size)
```

---
# Example 2: Summarize

```{r, message = FALSE, fig.align='center', fig.width=10}
summ <- normal_sims %>% group_by(sample_size, effect_size) %>%
    summarize(power = mean(detected)) %>%
    mutate("Effect Size" = as.factor(effect_size))

summ %>% ggplot() + 
    geom_point(aes(x = sample_size, y = power, col = `Effect Size`, shape = `Effect Size`)) + 
    geom_line(aes(x = sample_size, y = power, col = `Effect Size`)) + 
    xlab("Sample Size") + 
    ylab("Power") + 
    theme_bw() + 
    theme(axis.text = element_text(size = 16), 
          legend.text = element_text(size =16), 
          axis.title = element_text(size = 18), 
          legend.title = element_text(size = 18))
```


---
# Simulation Coding Tips

- Use functions so your code is modular. 

- Always set a seed so you can replicate your results. 

- Test as you go. 

- There are some software packages that automate the "boring" part (the loop). 

  - [DSC is my favorite](https://stephenslab.github.io/dsc-wiki/overview)
  - Recommend only if you are already a little familiar with running simulations and willing to learn a new interface. 
  - If you use simulations in your work outside this class, I highly recommend. 
  - Automates lots of issues like submitting jobs to the cluster. 
  - Makes it easy to add new analysis methods to existing simulations. 
  - Makes it easy to keep track of what you did. 
  
